{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df65269",
   "metadata": {},
   "source": [
    "## Tutorial: Preparing Movie Review Text for Binary Classification\n",
    "\n",
    "This week's focus is on learning how to work with real-world text data — specifically, full movie reviews pulled from websites like RogerEbert.com. Before we can build machine learning models next week, we need to understand how to clean, organize, and extract meaning from raw text.\n",
    "\n",
    "Last week, we used Covid testing data, where we used test results as the target variable (what we're predicting) and symptoms or other predictive values (such as high risk occupation or contacts) as features (what we use to train a model to predict the target variable). \n",
    "\n",
    "We can take a similar approach to document (text) classification, by converting documents into numerical  representations of words and phrases in each document, providing a vectorized version of a text document through frequency of particular tokens (words or phrases). \n",
    "\n",
    "This workshop will provide background for working with text data. \n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "1. Understand and extract review content (feature data)  \n",
    "   We'll locate and extract the main body of the review from an HTML page. This is the core descriptive text we'll use later as input for a classification model.\n",
    "\n",
    "2. Assign a simple binary label (target data)  \n",
    "   For each review, we’ll decide if it is positive (1) or negative (0), using clues like star ratings or the overall sentiment of the text. These labels won’t be used for training this week, but it’s important to understand where they come from.\n",
    "\n",
    "\n",
    "### Why Text Data Presents Additional Challenges\n",
    "\n",
    "- Text doesn't naturally present itself in a tabular, feature-based format like the Covid testing data. We need to find a way to represent text as a vector that may not be as immediatley intuitive.\n",
    "- Text from sournces on the web often includs noise like navigation bars, ads, and unrelated metadata  \n",
    "- Opinion and sentiment are expressed through nuance and word choice  \n",
    "- You can't use it as-is with models until it’s cleaned and transformed\n",
    "\n",
    "In this workship we will learn techniqyes to/:\n",
    "\n",
    "- Extracting useful content from raw HTML pages  \n",
    "- Cleaning and normalizing unstructured text (removing tags, lowering case, stripping whitespace)  \n",
    "- Thinking critically about what is signal versus what is noise in a document  \n",
    "- Preparing the data so it’s ready for modeling next week\n",
    "\n",
    "By the end of this exercise, you will have a cleaned dataset of movie reviews and labels that can be used for training a binary classifier — just not yet. First, let’s get the text ready.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0b7766",
   "metadata": {},
   "source": [
    "## Fetching the Raw HTML of a Web Page\n",
    "\n",
    "In this code, we use the `requests` library to fetch the raw HTML content of a web page. Specifically, we are retrieving the content from a review of the movie *Marmaduke* (2010) on RogerEbert.com. The code does the following:\n",
    "\n",
    "1. **Sends an HTTP GET request** to the specified URL using `requests.get()`.\n",
    "2. **Checks the response status** to ensure the page was successfully retrieved (status code `200` means success).\n",
    "3. **Extracts the raw HTML** of the page and stores it as a string in `page_source`.\n",
    "4. **Prints the first 100 characters** of the HTML content for inspection, which can help us see the structure or start of the page.\n",
    "5. **Handles errors** by printing a failure message if the page cannot be retrieved.\n",
    "\n",
    "This is useful for fetching web pages to process their content further, such as parsing the text of a review or extracting specific elements from the HTML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07b25f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html lang=\"en-US\" prefix=\"og: https://ogp.me/ns#\">\n",
      "<head>\n",
      "\t<meta charset=\"UTF-8\">\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://www.rogerebert.com/reviews/marmaduke-2010'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    page_source = response.text\n",
    "    print(page_source[:100])\n",
    "else:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aaa27e",
   "metadata": {},
   "source": [
    "## Extracting Clean Text from a Web Page\n",
    "\n",
    "The code above provides the raw page source, but it contains a lot of extraneous tags, symbols, and characters. How can we get a cleaner version?\n",
    "\n",
    "This code uses the `requests` and `BeautifulSoup` libraries to fetch and parse the HTML content of a web page. Specifically, it:\n",
    "\n",
    "1. Sends an HTTP GET request to the specified URL using `requests`.\n",
    "2. Parses the HTML content using `BeautifulSoup` with the `'html.parser'` parser.\n",
    "3. Extracts all visible text from the page (removing all HTML tags) using `.get_text()`.\n",
    "   - The `separator='\\n'` option adds line breaks between blocks of text for readability.\n",
    "   - The `strip=True` option removes leading and trailing whitespace from each block.\n",
    "4. Prints the resulting plain-text content to the console.\n",
    "\n",
    "> This is useful for extracting readable text from web pages, such as news articles or reviews, while ignoring HTML, JavaScript, and styling content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b5b3266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great Danes drool when they speak movie review (2010) | Roger Ebert\n",
      "Signup\n",
      "Search in https://www.rogerebert.com\n",
      "Movie Reviews\n",
      "Great Movies\n",
      "TV/Streaming\n",
      "Interviews\n",
      "Collections\n",
      "Roger Ebert\n",
      "Contributors\n",
      "Reviews\n",
      "Great Danes drool when they speak\n",
      "Comedy\n",
      "87 minutes\n",
      "‧\n",
      "PG\n",
      "‧\n",
      "2010\n",
      "Roger Ebert\n",
      "June 2, 2010\n",
      "3 min read\n",
      "20th Century-Fox presents  a film directed by\n",
      "Tom Dey\n",
      ". Written by Tim Rasmussen and Vince DiMeglio, based on the comic strip created by\n",
      "Brad Anderson\n",
      "and Phil Leeming. Running time: 87 minutes.\n",
      "Rated PG\n",
      "(for rude humor and language).\n",
      "Dogs cannot talk. This we know. Dogs can talk in the movies. This we also know. But when we see them lip-synching with their dialogue, it’s just plain grotesque. The best approach is the one used by “Garfield” in which we saw the cat and heard\n",
      "Bill Murray\n",
      ", but there was no nonsense about Garfield’s mouth moving.\n",
      "The moment I saw Marmaduke’s big drooling lips moving, I knew I was in trouble. There is nothing discreet about a Great Dane with a lot on his\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.rogerebert.com/reviews/marmaduke-2010'\n",
    "\n",
    "# Send GET request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check for success\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract visible text\n",
    "    movie_review_text = soup.get_text(separator='\\n', strip=True)\n",
    "    print(movie_review_text[:1000])  # Or write to file if needed\n",
    "else:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657fdcc",
   "metadata": {},
   "source": [
    "## Text Preprocessing Steps\n",
    "\n",
    "Text data is inherently messy and unstructured, often containing noise, inconsistencies, and irrelevant information. Raw text can be challenging for machine learning algorithms to interpret without some cleaning and preparation. The goal of preprocessing is to:\n",
    "- **Ensure consistency**: For example, converting text to lowercase so that words like \"Dog\" and \"dog\" are treated as the same.\n",
    "- **Reduce noise**: Removing unwanted characters like punctuation and stop words helps the model focus on the more meaningful parts of the text.\n",
    "- **Extract useful features**: By tokenizing the text and removing common, unimportant words, we help the model focus on the content that carries more significance.\n",
    "- **Normalize text**: Steps like stemming or lemmatization reduce different forms of words to their root form, improving the model's ability to compare words and make accurate predictions.\n",
    "\n",
    "### What We’ll Do:\n",
    "1. Lowercase the Text: To ensure consistency and avoid treating the same word differently because of capitalization.\n",
    "2. Remove Punctuation and Special Characters: To clean up unnecessary characters that don’t contribute to meaningful analysis.\n",
    "3. Tokenize the Text: Split the text into individual words or tokens, which are the basic units for analysis.\n",
    "4. Remove Stop Words: Filter out common words like \"the,\" \"is,\" \"and,\" which don’t carry significant meaning in most contexts.\n",
    "5. Stemming or Lemmatization: Reduce words to their root form to group similar words together and improve model performance.\n",
    "6. Optional Noise Removal: Clean up numbers, non-alphabetic characters, and extra whitespace to make the data even cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "983ca9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d425568b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/boushey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/boushey/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b763c11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original sentence\n",
    "text = \"Dogs are running in the park, and they ran fast! The dog was excited; it's their favorite spot.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c69fc835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs are running in the park, and they ran fast! the dog was excited; it's their favorite spot.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Normalize Capitalization\n",
    "normalized_text = text.lower()\n",
    "print(normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62b5071c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dogs', 'running', 'park,', 'ran', 'fast!', 'dog', 'excited;', 'favorite', 'spot.']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Remove Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in normalized_text.split() if word not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3f00c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dogs', 'running', 'park', 'ran', 'fast', 'dog', 'excited', 'favorite', 'spot']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Remove Punctuation (after stop word removal)\n",
    "text_without_punctuation = [re.sub(r'[^\\w\\s]', '', word) for word in filtered_tokens]\n",
    "print(text_without_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3aff9030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'run', 'park', 'run', 'fast', 'dog', 'excite', 'favorite', 'spot']\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word, pos='v') for word in text_without_punctuation]\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "639c9f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'run', 'park', 'run', 'fast', 'dog', 'excit', 'favorit', 'spot']\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in lemmatized_tokens]\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f4a2164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Step 1: Normalize Capitalization\n",
    "    normalized_text = text.lower()\n",
    "\n",
    "    # Step 2: Remove Stop Words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in normalized_text.split() if word not in stop_words]\n",
    "\n",
    "    # Step 3: Remove Punctuation\n",
    "    text_without_punctuation = [re.sub(r'[^\\w\\s]', '', word) for word in filtered_tokens]\n",
    "\n",
    "    # Step 4: Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word, pos='v') for word in text_without_punctuation]\n",
    "\n",
    "    # Step 5: Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in lemmatized_tokens]\n",
    "    \n",
    "    # Return final processed text\n",
    "    #return ' '.join(stemmed_tokens)\n",
    "    return ' '.join(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3151bddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great danes drool speak movie review 2010  roger ebert signup search httpswwwrogerebertcom movie review great movies tvstreaming interview collections roger ebert contributors review great danes drool speak comedy 87 minutes  pg  2010 roger ebert june 2 2010 3 min read 20th centuryfox present film direct tom dey  write tim rasmussen vince dimeglio base comic strip create brad anderson phil leeming run time 87 minutes rat pg for rude humor language dog cannot talk know dog talk movies also know see lipsynching dialogue its plain grotesque best approach one use garfield saw cat hear bill murray  nonsense garfields mouth move moment saw marmadukes big drool lips move know trouble nothing discreet great dane lot mind especially hes narrator film never shut up master phil move winslow family kansas orange county join crowd dog park vegetarian pet food company well say movie speak part dog and cat humans congenial pgrated animal comedy like comic strip 56th year maybe youll like it maybe not marmadukes personality isnt nearly engage garfields again personality youre market for maybe shouldnt consider lipsynched talk animal comedy first place plot california marmaduke like new back yard get hot water family dumb reason run away mazie collie hes romance park go search him rain marmaduke get lose family pile station wagon search too  long story make short  end burst sewer cause big sink hole although big one guatemala mazie fall in marmaduke leap her theyre sweep sewer come aqueduct phil winslow  lee pace  leap in forth great danes best friends gift comedians mazie typecast sexy collie couldnt pug play female lead little nontraditional cast speak that whats william h macy owner pet food company admire macy do imagine dozens ways could funny pet food tycoon movie sidestep play role right middle businessman hire macy first place  enough write read review talk animal movie little kid may like it its offensive dont find marmaduke particularly photogenic thats me great danes look like extra elbow movie get two star could do little better marmaduke keep big mouth shut stream on power tweet share share pin roger ebert roger ebert film critic chicago suntimes 1967 death 2013 1975 pulitzer prize distinguish criticism marmaduke comedy 87 minutes  pg  2010 cast steve coogan raisin sam elliott chupadogra owen wilson marmaduke kiefer sutherland bosco christopher giuseppe judy greer debbie winslow george lopez carlos lee pace phil winslow emma stone mazie leave comment please enable javascript view comment power disqus play dangerous animals brian tallerico white fear cortlyn kelly highest 2 lowest robert daniels phoenician scheme brian tallerico eddington brian tallerico bono stories surrender robert daniels deaf president now matt zoller seitz sister midnight peyton robinson kiss 2025 nell minow desert namibia monica castillo damn 2025 peter sobczynski love 2025 simon abrams latest article festivals  award cannes 2025 table content 7 hours ago interview movies deal peculiarities teenager bruce handy new book teen movies 7 hours ago festivals  award cannes 2025 exit 8 eleanor great fuori 9 hours ago festivals  award cannes 2025 renoir sons neon night orwell 225 9 hours ago best movie review inbox movie review rogers greatest movies review cast crew ebert prime sign movie genres action amazon prime comedy documentary drama horror hulu mystery netflix romance science fiction suspense thriller blog review chazs journal great movies mzs far flungers interview tributes video game black writers week feature tvstreaming roger ebert festivals  award ebert co site contact us advertise us contributors roger ebert love movies memoriam 1942  2013 ebert digital llc  copyright 2025 term use privacy policy'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(movie_review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd957a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
